{"version":"1","records":[{"hierarchy":{"lvl1":"Welcome to the EOxHub Workspaces Documentation"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Welcome to the EOxHub Workspaces Documentation"},"content":"This documentation is your starting point for exploring and utilizing\nEOxHub Workspaces, a powerful environment designed to support scalable,\ncloud-native geospatial workflows.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Welcome to the EOxHub Workspaces Documentation","lvl2":"What You’ll Find Here"},"type":"lvl2","url":"/#what-youll-find-here","position":2},{"hierarchy":{"lvl1":"Welcome to the EOxHub Workspaces Documentation","lvl2":"What You’ll Find Here"},"content":"The documentation is structured to guide users through a variety of resources\nand examples, including:\n\nUse Case DescriptionsReal-world scenarios and problem statements that EOxHub Workspaces is\ndesigned to address. These will help you understand the expected applications\nand capabilities of the service.\n\nTutorials and WalkthroughsStep-by-step guides to help new users get started and experienced users\nexplore advanced features.\n\nArgo Workflow TemplatesTemplates and patterns to automate and orchestrate your geospatial processing\npipelines using Argo Workflows.","type":"content","url":"/#what-youll-find-here","position":3},{"hierarchy":{"lvl1":"Welcome to the EOxHub Workspaces Documentation","lvl2":"Integrated Applications"},"type":"lvl2","url":"/#integrated-applications","position":4},{"hierarchy":{"lvl1":"Welcome to the EOxHub Workspaces Documentation","lvl2":"Integrated Applications"},"content":"The platform also provides access to a suite of integrated tools and services,\nsuch as:\n\nJupyter Lab – For data access and algorithm development.\n\npygeoapi – For serving geospatial data and processes through standardized OGC APIs.\n\nCredentials Manager – To handle access to external and internal resources securely and efficiently.\n\n...and more – Additional applications and services will be documented here as they become available or are integrated into the platform.\n\nWhether you’re a data scientist, developer, or decision-maker, this documentation\nis intended to support your journey with EOxHub Workspaces. Check back regularly\nas we continue to expand the content with new features, examples, and integrations.","type":"content","url":"/#integrated-applications","position":5},{"hierarchy":{"lvl1":"Argo"},"type":"lvl1","url":"/argo","position":0},{"hierarchy":{"lvl1":"Argo"},"content":"EOxHub uses \n\nArgo Workflows as its workflow orchestration tool, providing a solution for defining and executing multi-step processing pipelines where each step runs in its own container.\n\nThis enables scalable and repeatable execution of applications, supporting both simple tasks and complex, long-running jobs.\n\nA web-based workflow editor is included, offering operators an intuitive interface to design, manage, and visualize workflows.\n\n\n\nFigure 1:Argo Workflow Templates input mapping in web editor\n\nApplications are published in the platform as Docker images, which can be versioned and reused across different workflows. Argo’s native support for dynamic parameter passing and conditional logic enables the creation of modular pipelines.\n\nIn addition to on-demand execution, workflows can be scheduled for automated, repeated execution using cron triggers.\n\n\n\nFigure 2:Argo Workflow server\n\nArgo Workflows also provides detailed logs for all executions, enabling workspace administrator to monitor behavior, trace failures, and debug issues.\n\n\n\nFigure 3:Argo Workflows details of a finished workflow with details of a step\n\nSample templates will become available in the tutorial section once possible","type":"content","url":"/argo","position":1},{"hierarchy":{"lvl1":"Argo","lvl2":"Capability requirements for integration"},"type":"lvl2","url":"/argo#capability-requirements-for-integration","position":2},{"hierarchy":{"lvl1":"Argo","lvl2":"Capability requirements for integration"},"content":"Docker image/capability requirements for integration are described on the \n\nseparate page","type":"content","url":"/argo#capability-requirements-for-integration","position":3},{"hierarchy":{"lvl1":"Argo","lvl2":"Argo workflow steps"},"type":"lvl2","url":"/argo#argo-workflow-steps","position":4},{"hierarchy":{"lvl1":"Argo","lvl2":"Argo workflow steps"},"content":"Each workflow includes usually following types of steps:\n\nWorkflow input parameters and configuration of secrets (e.g., API tokens) as environment variables.\n\nDefinition of processing steps, such as data retrieval, transformation, analysis, and result management.\n\nInter-step communication via shared persistent storage for handling intermediate data between containers.","type":"content","url":"/argo#argo-workflow-steps","position":5},{"hierarchy":{"lvl1":"Argo","lvl2":"Data retrieval"},"type":"lvl2","url":"/argo#data-retrieval","position":6},{"hierarchy":{"lvl1":"Argo","lvl2":"Data retrieval"},"content":"Examples of data retrieval approaches include:\n\nThe container directly handles the download or access to external or internal data sources.\n\nData is passed from a previous workflow step.\n\nData already exists in the shared storage. It may have been placed there by scheduled (e.g. daily) Argo workflows or manually.","type":"content","url":"/argo#data-retrieval","position":7},{"hierarchy":{"lvl1":"Argo","lvl2":"Result management"},"type":"lvl2","url":"/argo#result-management","position":8},{"hierarchy":{"lvl1":"Argo","lvl2":"Result management"},"content":"Result management of the workflow may include:\n\nWriting output files to client-controlled storage buckets or centralized handover locations.\n\nReturning signed URLs or object storage paths as outputs for M2M scenarios. These can be persisted and exposed through pygeoapi interfaces, e.g., as utilized as a structured JSON as utilized by eodash processing widget.\n\nAutomatic registration of outputs along with necessary metadata as STAC Items in a workspace STAC catalog or a STAC API.","type":"content","url":"/argo#result-management","position":9},{"hierarchy":{"lvl1":"Argo","lvl2":"Metrics"},"type":"lvl2","url":"/argo#metrics","position":10},{"hierarchy":{"lvl1":"Argo","lvl2":"Metrics"},"content":"\n\nFigure 4:Argo Workflow Server built in resource usage metrics","type":"content","url":"/argo#metrics","position":11},{"hierarchy":{"lvl1":"Integration Requirements for Argo Workflows on EOxHub"},"type":"lvl1","url":"/capability-integration","position":0},{"hierarchy":{"lvl1":"Integration Requirements for Argo Workflows on EOxHub"},"content":"This document is intended as support material to describe how to best provide a capability implementation and its description in order to allow EOX to integrate it as a workflow inside of an EOxHub Workspace. This includes input definitions, expected Docker image standards, and output formats.","type":"content","url":"/capability-integration","position":1},{"hierarchy":{"lvl1":"Integration Requirements for Argo Workflows on EOxHub","lvl2":"Input Requirements"},"type":"lvl2","url":"/capability-integration#input-requirements","position":2},{"hierarchy":{"lvl1":"Integration Requirements for Argo Workflows on EOxHub","lvl2":"Input Requirements"},"content":"To ensure compatibility with various interfaces (e.g. OGC Process API) to allow on demand execution of the capability some input requirements need to be considered. Furthermore information providing more context is needed regarding following points:\n\nDescription of expected input data - does the service expect data already present or data are downloaded as part of the dockerized algorithm?\n\nDescription of expected argument and parameters\n\nDescription of expected output - see Output requirements section","type":"content","url":"/capability-integration#input-requirements","position":3},{"hierarchy":{"lvl1":"Integration Requirements for Argo Workflows on EOxHub","lvl3":"Arguments and parameters","lvl2":"Input Requirements"},"type":"lvl3","url":"/capability-integration#arguments-and-parameters","position":4},{"hierarchy":{"lvl1":"Integration Requirements for Argo Workflows on EOxHub","lvl3":"Arguments and parameters","lvl2":"Input Requirements"},"content":"eodash processing widget support multiple ways how to pass input.\n\nArea/location - process can take as input drawn point or polygon from the eodash user interface. For this integration, input field must accept eather coorinates directly or geoJSON as a string. File input is not accepted.\n\nExample GeoJSON Feature String ‘{“type”:“Feature”,“geometry”:{“type”:“Polygon” “coordinates”:[[[30,10],[40,40],[20,40],[10,20],[30,10]]]},“properties”:{}}’\n\nDate - standard HTML Format date formats are supported. eodash also supports start and end time to create range.\n\n“YYYY-MM-DD” e.g. 2015-05-30 for date.\n\n“YYYY-MM-DDThh:mm” for datetime 2025-07-02T06:33\n\nNumeric fields - integer or float values\n\nText fields\n\nDropdowns with limited options\n\nAll fields can have default value","type":"content","url":"/capability-integration#arguments-and-parameters","position":5},{"hierarchy":{"lvl1":"Integration Requirements for Argo Workflows on EOxHub","lvl2":"Docker Image Requirements"},"type":"lvl2","url":"/capability-integration#docker-image-requirements","position":6},{"hierarchy":{"lvl1":"Integration Requirements for Argo Workflows on EOxHub","lvl2":"Docker Image Requirements"},"content":"Your process must be encapsulated in a Docker image. There are no strict limitations of what can be done, but there are good practices that help integration:\n\nimage is “slim” - only with required dependencies installed\n\nimage is tagged with version based on \n\nsemantic versioning\n\nalgorithm logs to standard out (stdout) helping debug potential issues\n\nsimpler if no sideloading/sidecars (docker in docker) and similar\n\nno special volumes, network expectations, ...\n\nWe expect to know resource usage estimation - RAM consumption and CPU estimates.","type":"content","url":"/capability-integration#docker-image-requirements","position":7},{"hierarchy":{"lvl1":"Integration Requirements for Argo Workflows on EOxHub","lvl2":"Output Requirements"},"type":"lvl2","url":"/capability-integration#output-requirements","position":8},{"hierarchy":{"lvl1":"Integration Requirements for Argo Workflows on EOxHub","lvl2":"Output Requirements"},"content":"The workflow should store its results in /output folder, which will be collected automatically by our processes.\n\nExpected outputs:\n\nResults (e.g., COG, geoJSON, CSV)\n\nInputs are expected to be cloud-native so for raster output formats, COGs are expected, for vector data geoJSON for small files up to 5Mb or flatGeobuf\n\nLogs (optional but encouraged)\n\nOutputs must be:\n\nWritten with unique filenames if multiple files are generated.\n\nValidated (e.g., for COG format or JSON schema compliance).","type":"content","url":"/capability-integration#output-requirements","position":9},{"hierarchy":{"lvl1":"Integration Requirements for Argo Workflows on EOxHub","lvl2":"Sample Checklist"},"type":"lvl2","url":"/capability-integration#sample-checklist","position":10},{"hierarchy":{"lvl1":"Integration Requirements for Argo Workflows on EOxHub","lvl2":"Sample Checklist"},"content":"To submit your process for integration, make sure these information are included:\n\nDockerfile included and builds correctly\n\nREADME with usage instructions and input/output description\n\nSample call example\n\nLicense file / link to repository (not mandatory)\n\nSample input/output for testing\n\nVisualisation expectations\n\nPlease contact the EOxHub team or submit a ticket if youneed more information.","type":"content","url":"/capability-integration#sample-checklist","position":11},{"hierarchy":{"lvl1":"Conda Store"},"type":"lvl1","url":"/conda-store","position":0},{"hierarchy":{"lvl1":"Conda Store"},"content":"Within each user workspace where JupyterLab is provisioned also a dedicated \n\nConda Store is available.\n\nConda Store enables users to define and manage reproducible Python environments. This ensures consistent execution across sessions and allows developers to control the underlying libraries and dependencies for their workflows.\n\n\n\nFigure 1:Conda store User interface\n\nDedicated environments can be produced based on the user rights either for a user themselves or for a whole workspace and shared with colleagues or other people.\n\nConda Store supports GUI or yaml syntax for environment creation and examples and how to guides can be found in the official \n\ndocumentation\n\n\n\nFigure 2:Conda store environment creation","type":"content","url":"/conda-store","position":1},{"hierarchy":{"lvl1":"Data Editor"},"type":"lvl1","url":"/data-editor","position":0},{"hierarchy":{"lvl1":"Data Editor"},"content":"The Data Editor application, provides a traceable review and approval path of collection configurations before data is published to the configured STAC catalog. This STAC catalog is used in the \n\nPublishing Dashboard which is based on \n\neodash.\n\nIt is based on \n\ngit-clerk - Open-Source Content Management System based on Git workflows with a friendly file-editing GUI.\n\nIt enables workspace owners to describe their datasets using simple forms, validate them against JSON schema definitions, and commit them via Git-based sessions.\n\nData Editor collaborative publishing diagram 1flowchart LR\n  A[User <br> Git Clerk UI] --> B[Create Session]\n  B --> C[Write Narrative<br/>in Online Editor]\n  C --> D[Integrate <br> Maps <br> Charts]\n  D --> E[Preview <br> Draft Content]\n  E --> F{Is Content Ready?}\n  F -- No --> C\n  F -- Yes --> G[Submit]\nstyle A fill:#f9f,stroke:#333,stroke-width:2px\n\nData Editor collaborative publishing diagram 2flowchart LR\n  G[Submit] --> H[Manager <br> Reviews PR]\n  H --> I{Approve or Request Changes?}\n  I -- Request Changes --> C[Write Narrative<br/>in Online Editor]\n  I -- Approve --> J[Merge Pull Request]\n  J --> K[Narrative Added to Catalog<br/>via GitHub Actions]\n  G --> L[Public Preview Link Available]\n  L --> M[Share with 3rd Party for Feedback]\n  M --> G\n\nstyle K fill:#bbf,stroke:#333,stroke-width:2px\n\n\n\nFigure 1:Data Editor schema validation for a new collection","type":"content","url":"/data-editor","position":1},{"hierarchy":{"lvl1":"Data Editor","lvl2":"Supported data types"},"type":"lvl2","url":"/data-editor#supported-data-types","position":2},{"hierarchy":{"lvl1":"Data Editor","lvl2":"Supported data types"},"content":"Currently supported data (resource) types are:\n\nRaw sources:\n\nCOG source\n\nGeoJSON source\n\nFlatGeobuf source\n\nSentinelHub\n\nSentinelHub WMS\n\nGeoDB\n\nGeoDB Vector Tile\n\nWMS\n\nVEDA - Visualization, Exploration, and Data Analysis\n\nXCube Server\n\nCopernicus Marine Data Store WMTS\n\nList of all supported resources is kept up to date on \n\neodash wiki so please visit this site as well.","type":"content","url":"/data-editor#supported-data-types","position":3},{"hierarchy":{"lvl1":"Data Editor","lvl2":"Required information"},"type":"lvl2","url":"/data-editor#required-information","position":4},{"hierarchy":{"lvl1":"Data Editor","lvl2":"Required information"},"content":"All required fields are marked in the Data Editor. More information about each of the fields is available on the \n\nGitHub wiki page","type":"content","url":"/data-editor#required-information","position":5},{"hierarchy":{"lvl1":"Data Editor","lvl2":"Overview of the process"},"type":"lvl2","url":"/data-editor#overview-of-the-process","position":6},{"hierarchy":{"lvl1":"Data Editor","lvl2":"Overview of the process"},"content":"Generally for including a supported type of EO collection into an eodash deployment within EOxHub, the steps are summarized as follows:\n\nStart a new session in the Data Editor and create a new collection configuration file.\n\nFill metadata fields split into thematic groups. Mainly filling the \n\nResource is important to visualize the data on the web map.\n\nFor raw data and client only rendering (GeoJSON, flatgeobuf or GeoTIFF as direct access), eodash supports an \n\nOpenLayers flatstyle. More information on styling can be \n\nfound here.\n\neodash project offers \n\neodash-style-editor for editing the flatstyle definitions with updating the visualization in real time when definition is changed\n\nTo define interactions for the user (e.g. modify the style within the eodash app), the style can be extended with variables, combined with \n\nJSON Form definition.\n\nAfter finishing the updates confirmed by the layer live preview panel and approving of the corresponding Data editing session (GitHub Pull Request), the changes can be merged to production catalog.\n\nFor a more hands-on tutorial on how to publish insights by exposing data see our tutorials \n\nIntegrating GeoJSON file and \n\nIntegrating WMTS service\n\nFor learning how to include your data in Narrative publication, read the section \n\nNarrative Editor and follow the tutorial \n\nCreating Narrative","type":"content","url":"/data-editor#overview-of-the-process","position":7},{"hierarchy":{"lvl1":"eoAPI"},"type":"lvl1","url":"/eoapi","position":0},{"hierarchy":{"lvl1":"eoAPI"},"content":"EOxHub includes robust data visualization capabilities for both raster and vector data based primarily on \n\neoapi. These are powered by \n\ntitiler-pgstac and \n\nPgSTAC, which together enable dynamic tiling and rendering of STAC-compliant assets directly from storage based on pre-defined collection-level metadata fields.\n\nIt exposes a \n\nFastAPI based interface, supporting image formats such as PNG and JPEG, and standard interfaces like WMTS.\n\nFuture tutorials for eoAPI will become available in the tutorial section once possible.","type":"content","url":"/eoapi","position":1},{"hierarchy":{"lvl1":"eoAPI","lvl2":"Functionality"},"type":"lvl2","url":"/eoapi#functionality","position":2},{"hierarchy":{"lvl1":"eoAPI","lvl2":"Functionality"},"content":"","type":"content","url":"/eoapi#functionality","position":3},{"hierarchy":{"lvl1":"eoAPI","lvl3":"Catalog Your Data","lvl2":"Functionality"},"type":"lvl3","url":"/eoapi#catalog-your-data","position":4},{"hierarchy":{"lvl1":"eoAPI","lvl3":"Catalog Your Data","lvl2":"Functionality"},"content":"PgSTAC is an optimized Postgres schema to index and search large-scale STAC collections.\n\nIngesting the existing STAC items to the database can be done in multiple ways:\n\nManually via \n\npypgstac as pypgstac load items\n\nManually if \n\ntransaction extension is enabled, then STAC data can be pushed to the API endpoint directly using POST/PUT requests.","type":"content","url":"/eoapi#catalog-your-data","position":5},{"hierarchy":{"lvl1":"eoAPI","lvl3":"Make it Searchable","lvl2":"Functionality"},"type":"lvl3","url":"/eoapi#make-it-searchable","position":6},{"hierarchy":{"lvl1":"eoAPI","lvl3":"Make it Searchable","lvl2":"Functionality"},"content":"This service utilizes stac-fastapi to publish and manage metadata describing their datasets, enabling machine-readable search, query, filter and cataloging across collections.","type":"content","url":"/eoapi#make-it-searchable","position":7},{"hierarchy":{"lvl1":"eoAPI","lvl3":"Visualize Raster Data","lvl2":"Functionality"},"type":"lvl3","url":"/eoapi#visualize-raster-data","position":8},{"hierarchy":{"lvl1":"eoAPI","lvl3":"Visualize Raster Data","lvl2":"Functionality"},"content":"titiler-pgSTAC is a TiTiler extension that connects to pgSTAC to support large-scale dynamic mosaic tiling for visualizing STAC collections and items.","type":"content","url":"/eoapi#visualize-raster-data","position":9},{"hierarchy":{"lvl1":"eoAPI","lvl3":"Visualize Vector Data","lvl2":"Functionality"},"type":"lvl3","url":"/eoapi#visualize-vector-data","position":10},{"hierarchy":{"lvl1":"eoAPI","lvl3":"Visualize Vector Data","lvl2":"Functionality"},"content":"Utilizing \n\ntipg - Vector Tiling Service  for OGC Features and OGC Tiles specifications.","type":"content","url":"/eoapi#visualize-vector-data","position":11},{"hierarchy":{"lvl1":"File Browser"},"type":"lvl1","url":"/file-browser","position":0},{"hierarchy":{"lvl1":"File Browser"},"content":"The File Browser is a web-based file management application integrated into the EOxHub Workspace, enabling users to efficiently upload, organize, and manage geospatial data assets. Built upon the \n\nVersioneer Tech Package-R, it offers a streamlined interface for interacting with datasets within the workspace.","type":"content","url":"/file-browser","position":1},{"hierarchy":{"lvl1":"File Browser","lvl2":"Key Features"},"type":"lvl2","url":"/file-browser#key-features","position":2},{"hierarchy":{"lvl1":"File Browser","lvl2":"Key Features"},"content":"User-Friendly Interface: Provides a clean, intuitive interface for navigating and managing files.\n\nFolder Structure Management: Allows users to create, rename, and organize folders to structure their data effectively.\n\nFile Operations: Supports standard file operations such as upload, download, delete, and move, facilitating efficient data management.\n\nMetadata Integration: Enables the association of metadata with files, enhancing data discoverability and context.","type":"content","url":"/file-browser#key-features","position":3},{"hierarchy":{"lvl1":"File Browser","lvl2":"Public Access System"},"type":"lvl2","url":"/file-browser#public-access-system","position":4},{"hierarchy":{"lvl1":"File Browser","lvl2":"Public Access System"},"content":"EOxHub Workspace employs a public access system to facilitate sharing and accessing datasets:\n\nPublic Folder: Files placed in the public directory are accessible via permanent public URLs, allowing for easy sharing and integration into other applications e.g. \n\nData Editor.\n\nShareable Links: Users can generate shareable links for specific files or folders, providing controlled access to external collaborators.\n\nAccess Control: All areas within the File Browser are restricted to be accessed only by users of the workspace. Data in the public folder is openly accessible. By using the creation of shareable links other files can be made accessible to external people.","type":"content","url":"/file-browser#public-access-system","position":5},{"hierarchy":{"lvl1":"Headless Execution"},"type":"lvl1","url":"/headless-execution","position":0},{"hierarchy":{"lvl1":"Headless Execution"},"content":"","type":"content","url":"/headless-execution","position":1},{"hierarchy":{"lvl1":"Headless Execution","lvl2":"Headless Execution in EDC"},"type":"lvl2","url":"/headless-execution#headless-execution-in-edc","position":2},{"hierarchy":{"lvl1":"Headless Execution","lvl2":"Headless Execution in EDC"},"content":"The Headless Execution feature in EOxHub Workspaces enables automated execution of Jupyter notebooks and Argo Workflows directly from the eodash dashboard or programmatically via API endpoints. It is designed for streamlined, reproducible, and user-friendly processing of Earth Observation tasks and workflows.","type":"content","url":"/headless-execution#headless-execution-in-edc","position":3},{"hierarchy":{"lvl1":"Headless Execution","lvl3":"What is Headless Execution?","lvl2":"Headless Execution in EDC"},"type":"lvl3","url":"/headless-execution#what-is-headless-execution","position":4},{"hierarchy":{"lvl1":"Headless Execution","lvl3":"What is Headless Execution?","lvl2":"Headless Execution in EDC"},"content":"Headless execution allows you to:\n\nTrigger processing jobs (e.g. notebooks, workflows) without manual interaction\n\nRun parameterized notebooks via API (e.g. different AOIs, time ranges, datasets)\n\nConnect dashboard buttons or UI elements directly to backend EO analysis pipelines\n\nMonitor job status and outputs centrally\n\nIt is particularly useful for:\n\nEnd-user-triggered tasks in EO dashboards\n\nScheduled or batch analyses\n\nLightweight data services","type":"content","url":"/headless-execution#what-is-headless-execution","position":5},{"hierarchy":{"lvl1":"Headless Execution","lvl3":"Argo Workflows & pygeoapi Integration","lvl2":"Headless Execution in EDC"},"type":"lvl3","url":"/headless-execution#argo-workflows-pygeoapi-integration","position":6},{"hierarchy":{"lvl1":"Headless Execution","lvl3":"Argo Workflows & pygeoapi Integration","lvl2":"Headless Execution in EDC"},"content":"EOxHub uses pygeoapi to expose Argo Workflows as standard OGC-compliant processes. This enables external tools or dashboards to:\n\nDiscover available jobs and workflows\n\nSubmit parameterized execution requests\n\nTrack status and retrieve results\n\nEach job has:\n\nA unique identifier and description\n\nA list of accepted parameters (e.g. AOI, date, dataset)\n\nExecution logs and outputs available via API or the EOxHub UI","type":"content","url":"/headless-execution#argo-workflows-pygeoapi-integration","position":7},{"hierarchy":{"lvl1":"Headless Execution","lvl3":"Triggering Notebook Jobs","lvl2":"Headless Execution in EDC"},"type":"lvl3","url":"/headless-execution#triggering-notebook-jobs","position":8},{"hierarchy":{"lvl1":"Headless Execution","lvl3":"Triggering Notebook Jobs","lvl2":"Headless Execution in EDC"},"content":"In addition to workflows, parameterized Jupyter notebooks can also be exposed for headless execution. This allows notebooks to be reused as backend processors while preserving reproducibility and transparency.","type":"content","url":"/headless-execution#triggering-notebook-jobs","position":9},{"hierarchy":{"lvl1":"Headless Execution","lvl3":"Monitoring and Managing Jobs","lvl2":"Headless Execution in EDC"},"type":"lvl3","url":"/headless-execution#monitoring-and-managing-jobs","position":10},{"hierarchy":{"lvl1":"Headless Execution","lvl3":"Monitoring and Managing Jobs","lvl2":"Headless Execution in EDC"},"content":"Once triggered, jobs can be tracked in the Headless Execution section of the workspace UI:\n\nView job queue and running/completed status\n\nInspect input parameters and output previews\n\nRe-run or cancel jobs if needed","type":"content","url":"/headless-execution#monitoring-and-managing-jobs","position":11},{"hierarchy":{"lvl1":"JupyterLab"},"type":"lvl1","url":"/jupyterlab","position":0},{"hierarchy":{"lvl1":"JupyterLab"},"content":"JupyterLab in the EOxHub Workspaces environment provides a flexible, browser-based interface for interactive computing, data analysis, and algorithm development. It is the primary workspace for working with Earth Observation (EO) data, executing Python code, and building reproducible workflows using Jupyter Notebooks.","type":"content","url":"/jupyterlab","position":1},{"hierarchy":{"lvl1":"JupyterLab","lvl2":"What is JupyterLab?"},"type":"lvl2","url":"/jupyterlab#what-is-jupyterlab","position":2},{"hierarchy":{"lvl1":"JupyterLab","lvl2":"What is JupyterLab?"},"content":"JupyterLab is a next-generation web-based user interface for Project Jupyter. It enables users to:\n\nWrite and run code in Jupyter Notebooks\n\nAccess a terminal and file browser\n\nView and edit CSVs, images, and text files\n\nUse drag-and-drop functionality across tabs\n\nIn the context or EOxHub Workspaces, JupyterLab comes pre-configured with common EO and geospatial libraries, making it ideal for analysis, visualization, and prototyping. For more information, please visit the official \n\ndocumentation","type":"content","url":"/jupyterlab#what-is-jupyterlab","position":3},{"hierarchy":{"lvl1":"JupyterLab","lvl2":"Starting with JupyterLab in EOxHub Workspaces"},"type":"lvl2","url":"/jupyterlab#starting-with-jupyterlab-in-eoxhub-workspaces","position":4},{"hierarchy":{"lvl1":"JupyterLab","lvl2":"Starting with JupyterLab in EOxHub Workspaces"},"content":"When launching JupyterLab in EOxHub, you will be asked to choose a user profile, which defines the computational resources available (RAM, CPU, and in some cases also disk space). This helps to tailor your session based on the workload.\n\nThese are examples of common profiles based on the chosen subscription plan:\n\nTrial Profile: Ideal for lightweight exploration and testing, usually avavilable in workshop settings or trials\n\nStandard Profile: Recommended for moderate EO processing\n\nLarge Profile: For heavy workloads (model training, large-scale analysis) or usage of GPPU\n\nIf your use case requires more resources, longer runtimes or GPU, please reach out to request a custom setup.","type":"content","url":"/jupyterlab#starting-with-jupyterlab-in-eoxhub-workspaces","position":5},{"hierarchy":{"lvl1":"JupyterLab","lvl2":"Special Kernels and Environments"},"type":"lvl2","url":"/jupyterlab#special-kernels-and-environments","position":6},{"hierarchy":{"lvl1":"JupyterLab","lvl2":"Special Kernels and Environments"},"content":"JupyterLab in EOxHub supports multiple custom kernels depending on your analysis needs. To learn how to install or request specific environments (e.g. for deep learning or domain-specific libraries), refer to the:\n\n➡️ \n\nConda Store Documentation section","type":"content","url":"/jupyterlab#special-kernels-and-environments","position":7},{"hierarchy":{"lvl1":"JupyterLab","lvl2":"Exploring Example Notebooks"},"type":"lvl2","url":"/jupyterlab#exploring-example-notebooks","position":8},{"hierarchy":{"lvl1":"JupyterLab","lvl2":"Exploring Example Notebooks"},"content":"\n\nTo get started quickly, navigate to the Examples Explorer section of the EOxHub Workspace. There, you’ll find:\n\nReady-to-run sample notebooks\n\nNotebook tutorials on data access and visualization\n\nSample Workflows covering EO analysis\n\nThese notebooks are an excellent entry point to understand EOxHub Workspaces, JupyterLab options, and data.","type":"content","url":"/jupyterlab#exploring-example-notebooks","position":9},{"hierarchy":{"lvl1":"Narrative Editor"},"type":"lvl1","url":"/narrative-editor","position":0},{"hierarchy":{"lvl1":"Narrative Editor"},"content":"The Narrative Editor is a content editor for creating and publishing structured narratives combined with Earth Observation (EO) content. This documentation provides an overview of its architecture, key features, and functionality.","type":"content","url":"/narrative-editor","position":1},{"hierarchy":{"lvl1":"Narrative Editor","lvl2":"Overview"},"type":"lvl2","url":"/narrative-editor#overview","position":2},{"hierarchy":{"lvl1":"Narrative Editor","lvl2":"Overview"},"content":"The Narrative Editor is based on the \n\nstorytelling EOXElement and includes a preview renderer that uses Markdown text along with additional metadata (frontmatter) to define story settings and structure.\n\nIt integrates:\n\nA Git-based workflow using git-clerk for content versioning and collaboration\n\nScrollytelling and paginated rendering of narratives — where content can be presented either as a continuous scrolling story or divided into discrete pages or sections for a step-by-step reading experience\n\nLive preview for story validation\n\nSupport for interactive map tours","type":"content","url":"/narrative-editor#overview","position":3},{"hierarchy":{"lvl1":"Narrative Editor","lvl2":"Key Features"},"type":"lvl2","url":"/narrative-editor#key-features","position":4},{"hierarchy":{"lvl1":"Narrative Editor","lvl2":"Key Features"},"content":"","type":"content","url":"/narrative-editor#key-features","position":5},{"hierarchy":{"lvl1":"Narrative Editor","lvl3":"Git-Based Editing","lvl2":"Key Features"},"type":"lvl3","url":"/narrative-editor#git-based-editing","position":6},{"hierarchy":{"lvl1":"Narrative Editor","lvl3":"Git-Based Editing","lvl2":"Key Features"},"content":"Users connect their EOxHub account to GitHub.\n\nEditing an existing narrative creates forks or branches.\n\nUsers can request reviews through pull requests in GitHub.","type":"content","url":"/narrative-editor#git-based-editing","position":7},{"hierarchy":{"lvl1":"Narrative Editor","lvl3":"Story Rendering","lvl2":"Key Features"},"type":"lvl3","url":"/narrative-editor#story-rendering","position":8},{"hierarchy":{"lvl1":"Narrative Editor","lvl3":"Story Rendering","lvl2":"Key Features"},"content":"Live preview of changes as they are made.\n\nSupports both paginated and scroll-style rendering.","type":"content","url":"/narrative-editor#story-rendering","position":9},{"hierarchy":{"lvl1":"Narrative Editor","lvl3":"Map Integration","lvl2":"Key Features"},"type":"lvl3","url":"/narrative-editor#map-integration","position":10},{"hierarchy":{"lvl1":"Narrative Editor","lvl3":"Map Integration","lvl2":"Key Features"},"content":"Supports eox-map \n\nEOxElement blocks.\n\nProvides a polished experience with seamless transitions and animations between map tour steps.\n\nCompatible with the \n\nEO Dashboard scrollytelling configuration.","type":"content","url":"/narrative-editor#map-integration","position":11},{"hierarchy":{"lvl1":"Narrative Editor","lvl2":"Workflow Summary"},"type":"lvl2","url":"/narrative-editor#workflow-summary","position":12},{"hierarchy":{"lvl1":"Narrative Editor","lvl2":"Workflow Summary"},"content":"Start Session: Create a new session to start creating or editing.\n\nCreate Narrative: Title your project and begin with a basic template.\n\nEdit Content: Use Markdown to make changes and view them live in the preview feature.\n\nSave and Review: Save locally, then submit a pull request on GitHub for review.\n\nMerge and Publish: Approved narratives are merged and deployed to the official narrative catalog.","type":"content","url":"/narrative-editor#workflow-summary","position":13},{"hierarchy":{"lvl1":"Narrative Editor","lvl2":"Reference Tutorial"},"type":"lvl2","url":"/narrative-editor#reference-tutorial","position":14},{"hierarchy":{"lvl1":"Narrative Editor","lvl2":"Reference Tutorial"},"content":"For a visual step-by-step guide, please refer to:\n➡️ \n\nAn introduction to the Narrative Editor","type":"content","url":"/narrative-editor#reference-tutorial","position":15},{"hierarchy":{"lvl1":"Narrative Editor","lvl2":"Technical Details"},"type":"lvl2","url":"/narrative-editor#technical-details","position":16},{"hierarchy":{"lvl1":"Narrative Editor","lvl2":"Technical Details"},"content":"The Narrative Editor provides collaborative editing, review, and approval of narratives before publication in the corresponding repository. It is based on \n\ngit-clerk—an open-source content management system built on Git workflows with a user-friendly file-editing interface.\n\nMarkdown files are organized into story blocks and, when needed, further divided into individual steps within each section. Rendered stories can be displayed in either a paginated format or scrollytelling mode.\n\nA \n\ndemo story demonstrates most of the current functionality.\n\nA notable feature is the built-in integration with the \n\neox-map element, which enables interactive map tours that smoothly transition between areas or layers as users scroll through the story.\n\neox-chart-element can be included in the stories as well.\n\n\n\nFigure 1:The Narrative Editor interface","type":"content","url":"/narrative-editor#technical-details","position":17},{"hierarchy":{"lvl1":"Narrative Editor","lvl2":"Additional Resources"},"type":"lvl2","url":"/narrative-editor#additional-resources","position":18},{"hierarchy":{"lvl1":"Narrative Editor","lvl2":"Additional Resources"},"content":"Storytelling Reference and Supported Features\n\nNarrative Examples Repository\n\ngit-clerk GitHub Repository\n\nFor development inquiries or integration support, please contact the EOX team.","type":"content","url":"/narrative-editor#additional-resources","position":19},{"hierarchy":{"lvl1":"Publishing Dashboard"},"type":"lvl1","url":"/publishing-dashboard","position":0},{"hierarchy":{"lvl1":"Publishing Dashboard"},"content":"The Publishing Dashboard provides a user-facing web interface for presenting and distributing curated datasets and narratives. It centers interactive maps and charts, as well as search and filter capabilities, and can be configured for public sharing or restricted access for defined user groups.\n\nDaily operation does not require coding skills or deep technical knowledge: content and configuration are maintained via the Data Editor and Narrative Editor, other applications available inside the EOxHub workspace together with the dashboard.\n\nThe Publishing Dashboard is based on eodash, EOxElements and vitepress. It is a web client instance connected to the information managed in the \n\nData Editor as well as the \n\nNarrative Editor.\n\nUsually the Publishing Dashboard is configured for public access, as it is intended to proliferate information to a larger audience.\nIf the intent is to provide data access to a managed user circle it is also possible to run the Publishing Dashboard within the EOxHub Workspace allowing only selected users access to the content. Multiple instances can be managed for different user groups within one workspace.\n\nInside the Dashboard section you can see the data configured in the Data Editor, as in the following Figure.\n\n\n\nFigure 1:Dashboard screenshot from \n\neodashboard.org\n\nThe created narratives can also be listed, searched and explored as soon as the content becomes available in the repository managed by the Narrative Editor.\n\n\n\nFigure 2:Narratives overview screenshot from \n\neodashboard.org\n\nFor more information on how to manage the published data content please have a look at the \n\nData Editor section, or for managing narratives at the \n\nNarrative Editor section.","type":"content","url":"/publishing-dashboard","position":1},{"hierarchy":{"lvl1":"Credentials Manager"},"type":"lvl1","url":"/secret-manager","position":0},{"hierarchy":{"lvl1":"Credentials Manager"},"content":"Workspace secrets—such as API tokens, database credentials, or service keys—are securely managed using the EOxHub Credentials Manager. This tool ensures sensitive information is not hard-coded in notebooks, workflows, or shared files.\n\n\n\nFigure 1:Credentials manager","type":"content","url":"/secret-manager","position":1},{"hierarchy":{"lvl1":"Credentials Manager","lvl2":"Key Features"},"type":"lvl2","url":"/secret-manager#key-features","position":2},{"hierarchy":{"lvl1":"Credentials Manager","lvl2":"Key Features"},"content":"Centralized and secure storage of credentials for all workspace users\n\nCollaborative access: All users within the workspace with correct roles can access and manage secrets\n\nReusability: Secrets can be reused across different applications (e.g., JupyterLab, Argo Workflows, data pipelines)","type":"content","url":"/secret-manager#key-features","position":3},{"hierarchy":{"lvl1":"Credentials Manager","lvl2":"Example Use Cases"},"type":"lvl2","url":"/secret-manager#example-use-cases","position":4},{"hierarchy":{"lvl1":"Credentials Manager","lvl2":"Example Use Cases"},"content":"Accessing a Sentinel Hub instance or other APIs directly from JupyterLab\n\nSupplying credentials to an Argo Workflow that fetches or publishes data to a cloud bucket\n\nConfiguring private dataset access within processing jobs","type":"content","url":"/secret-manager#example-use-cases","position":5},{"hierarchy":{"lvl1":"Credentials Manager","lvl2":"Credential types"},"type":"lvl2","url":"/secret-manager#credential-types","position":6},{"hierarchy":{"lvl1":"Credentials Manager","lvl2":"Credential types"},"content":"Opaque (default)\n\n(one or more) key-value pair(s)\n\ndockerconfigjson\n\nused to store authentication credentials for container image registries\n\nreferenced in pods as imagePullSecrets\n\nssh-auth\n\nused to store a private authentication key\n\nMore detailed information about these secret types can be found in the official documentation: \n\nhttps://​kubernetes​.io​/docs​/concepts​/configuration​/secret​/​#secret​-types","type":"content","url":"/secret-manager#credential-types","position":7},{"hierarchy":{"lvl1":"Credentials Manager","lvl3":"Creation","lvl2":"Credential types"},"type":"lvl3","url":"/secret-manager#creation","position":8},{"hierarchy":{"lvl1":"Credentials Manager","lvl3":"Creation","lvl2":"Credential types"},"content":"Type\n\nExample\n\nOpaque (key-value)   You can add one or more key-value pairs.\n\n\n\ndockerconfigjson  The provided credentials are converted to a valid json and stored in the secret.\n\n\n\nssh-auth  You can either upload the key file or paste the key in the text area.\n\n","type":"content","url":"/secret-manager#creation","position":9},{"hierarchy":{"lvl1":"Credentials Manager","lvl3":"Modification / Deletion","lvl2":"Credential types"},"type":"lvl3","url":"/secret-manager#modification-deletion","position":10},{"hierarchy":{"lvl1":"Credentials Manager","lvl3":"Modification / Deletion","lvl2":"Credential types"},"content":"Type\n\nExample\n\nOpaque (key-value)   \n\nadd / remove key-value pairs\n\nchange individual keys / values\n\n\n\ndockerconfigjson  The credentials can be changed, the current state of the currently stored json is shown in the (readonly) textarea below.\n\n\n\nssh-auth  The ssh-privatekey is masked and immutable once it’s stored. If you need to change the key, you need to delete the secret first and then recreate it.\n\n","type":"content","url":"/secret-manager#modification-deletion","position":11},{"hierarchy":{"lvl1":"Credentials Manager","lvl3":"Special modes","lvl2":"Credential types"},"type":"lvl3","url":"/secret-manager#special-modes","position":12},{"hierarchy":{"lvl1":"Credentials Manager","lvl3":"Special modes","lvl2":"Credential types"},"content":"Opaque (key-value) can be set to readonly or key-only (has to be done by an administrator).\nReadonly & key-only credentials can only be viewed, but not edited or deleted.\n\n\n\nFigure 2:Credential can only be viewed, not edited or deleted.\n\nMode\n\nExample\n\nreadonly\n\n\n\nkey-only\n\n","type":"content","url":"/secret-manager#special-modes","position":13},{"hierarchy":{"lvl1":"Credentials Manager","lvl3":"Inject as environment variables into Jupyterlab","lvl2":"Credential types"},"type":"lvl3","url":"/secret-manager#inject-as-environment-variables-into-jupyterlab","position":14},{"hierarchy":{"lvl1":"Credentials Manager","lvl3":"Inject as environment variables into Jupyterlab","lvl2":"Credential types"},"content":"Opaque (key-value) secrets can injected as environment variables in Jupyterlab by enabling the Inject as env var into jupyterlab button below the credential.\n\nIf you enable/disable this button you need to restart the Jupyterlab session to see its effect.\n\nInject as env into Jupyterlab\n\nExample\n\ndisabled\n\n\n\nenabled\n\n\n\nHow to access env it in Jupyterlab Notebook:\n\n","type":"content","url":"/secret-manager#inject-as-environment-variables-into-jupyterlab","position":15},{"hierarchy":{"lvl1":"What is EOxHub"},"type":"lvl1","url":"/intro","position":0},{"hierarchy":{"lvl1":"What is EOxHub"},"content":"","type":"content","url":"/intro","position":1},{"hierarchy":{"lvl1":"What is EOxHub","lvl2":"Welcome to EOxHub Workspaces"},"type":"lvl2","url":"/intro#welcome-to-eoxhub-workspaces","position":2},{"hierarchy":{"lvl1":"What is EOxHub","lvl2":"Welcome to EOxHub Workspaces"},"content":"EOxHub Workspaces is a cloud-based environment for Earth Observation (EO) professionals, researchers, and data scientists. An EOxHub Workspace is an online-accessible environment where individuals or teams can work with geospatial services and applications — all from a unified web interface. It is designed for flexibility, making it suitable for anyone who need pre-configured tools and cloud resources.","type":"content","url":"/intro#welcome-to-eoxhub-workspaces","position":3},{"hierarchy":{"lvl1":"What is EOxHub","lvl3":"Who is this for?","lvl2":"Welcome to EOxHub Workspaces"},"type":"lvl3","url":"/intro#who-is-this-for","position":4},{"hierarchy":{"lvl1":"What is EOxHub","lvl3":"Who is this for?","lvl2":"Welcome to EOxHub Workspaces"},"content":"Earth Observation scientists and remote sensing specialists\n\nData scientists working with geospatial or environmental data\n\nDevelopers building EO pipelines or prototypes\n\nEducators and students in the EO and data science fields","type":"content","url":"/intro#who-is-this-for","position":5},{"hierarchy":{"lvl1":"Getting Started with EOxHub Workspaces"},"type":"lvl1","url":"/getting-started","position":0},{"hierarchy":{"lvl1":"Getting Started with EOxHub Workspaces"},"content":"Welcome! This guide will walk you through the basic steps to start using EOxHub Workspaces.","type":"content","url":"/getting-started","position":1},{"hierarchy":{"lvl1":"Getting Started with EOxHub Workspaces","lvl2":"Sign Up"},"type":"lvl2","url":"/getting-started#sign-up","position":2},{"hierarchy":{"lvl1":"Getting Started with EOxHub Workspaces","lvl2":"Sign Up"},"content":"To use EOxHub Workspaces, all you need is an Internet browser.\n\nGo to \n\nhub.eox.at or navigate to the URL of your workspace\n\nEither click the Register button and follow the instructions or use one of the alternative log in methods like GitHub\n\nConfirm your email address following the link sent to you via email\n\nAgree to the terms and conditions\n\nWait for the confirmation by your workspace manager\n\nFor more detailed instructions, including screenshots, see our tutorial \n\nHow to create an account in EOxHub workspaces.","type":"content","url":"/getting-started#sign-up","position":3},{"hierarchy":{"lvl1":"Getting Started with EOxHub Workspaces","lvl2":"Log In"},"type":"lvl2","url":"/getting-started#log-in","position":4},{"hierarchy":{"lvl1":"Getting Started with EOxHub Workspaces","lvl2":"Log In"},"content":"After your account has been approved, you can log in anytime.\n\nNavigate to the URL of your workspace\n\nEither use your email address and password and click Sign In or use one of the alternative log in methods like GitHub","type":"content","url":"/getting-started#log-in","position":5},{"hierarchy":{"lvl1":"Getting Started with EOxHub Workspaces","lvl2":"Navigating the user area"},"type":"lvl2","url":"/getting-started#navigating-the-user-area","position":6},{"hierarchy":{"lvl1":"Getting Started with EOxHub Workspaces","lvl2":"Navigating the user area"},"content":"After signing in, you’ll see the user area with the tools available to your selected workspace. The list of actually available tools depends on the concrete workspace configuration. Their respective documentations can be found in the \n\nsecond chapter of this documentation.","type":"content","url":"/getting-started#navigating-the-user-area","position":7},{"hierarchy":{"lvl1":"Frequently asked questions"},"type":"lvl1","url":"/faq","position":0},{"hierarchy":{"lvl1":"Frequently asked questions"},"content":"","type":"content","url":"/faq","position":1},{"hierarchy":{"lvl1":"Frequently asked questions","lvl2":"Q: What is EOxHub Workspaces?"},"type":"lvl2","url":"/faq#q-what-is-eoxhub-workspaces","position":2},{"hierarchy":{"lvl1":"Frequently asked questions","lvl2":"Q: What is EOxHub Workspaces?"},"content":"A: EOxHub Workspaces is a cloud-based environment for exploring, analyzing, and publishing Earth Observation (EO) data using interactive notebooks and tools like JupyterLab.","type":"content","url":"/faq#q-what-is-eoxhub-workspaces","position":3},{"hierarchy":{"lvl1":"Frequently asked questions","lvl2":"Q: Who can use EOxHub Workspaces?"},"type":"lvl2","url":"/faq#q-who-can-use-eoxhub-workspaces","position":4},{"hierarchy":{"lvl1":"Frequently asked questions","lvl2":"Q: Who can use EOxHub Workspaces?"},"content":"A: Everyone with an internet connection! It’s designed for scientists, developers, educators, and anyone interested in working with EO data.","type":"content","url":"/faq#q-who-can-use-eoxhub-workspaces","position":5},{"hierarchy":{"lvl1":"Frequently asked questions","lvl2":"Q: Do I need to install anything?"},"type":"lvl2","url":"/faq#q-do-i-need-to-install-anything","position":6},{"hierarchy":{"lvl1":"Frequently asked questions","lvl2":"Q: Do I need to install anything?"},"content":"A: No. Everything runs in your browser. All required libraries and tools are pre-installed in the cloud workspace.","type":"content","url":"/faq#q-do-i-need-to-install-anything","position":7},{"hierarchy":{"lvl1":"Frequently asked questions","lvl2":"Q: Can I install additional python packages?"},"type":"lvl2","url":"/faq#q-can-i-install-additional-python-packages","position":8},{"hierarchy":{"lvl1":"Frequently asked questions","lvl2":"Q: Can I install additional python packages?"},"content":"A: Yes! You can install packages using conda store.","type":"content","url":"/faq#q-can-i-install-additional-python-packages","position":9},{"hierarchy":{"lvl1":"Frequently asked questions","lvl2":"Q: Will my work be saved?"},"type":"lvl2","url":"/faq#q-will-my-work-be-saved","position":10},{"hierarchy":{"lvl1":"Frequently asked questions","lvl2":"Q: Will my work be saved?"},"content":"A: Yes, files in your personal workspace are saved across sessions. However, we recommend backing up important work regularly.","type":"content","url":"/faq#q-will-my-work-be-saved","position":11},{"hierarchy":{"lvl1":"Frequently asked questions","lvl2":"Q: Can I upload my own data?"},"type":"lvl2","url":"/faq#q-can-i-upload-my-own-data","position":12},{"hierarchy":{"lvl1":"Frequently asked questions","lvl2":"Q: Can I upload my own data?"},"content":"A: Yes! You can upload files directly into your workspace.","type":"content","url":"/faq#q-can-i-upload-my-own-data","position":13},{"hierarchy":{"lvl1":"Frequently asked questions","lvl2":"Q: How can I get help?"},"type":"lvl2","url":"/faq#q-how-can-i-get-help","position":14},{"hierarchy":{"lvl1":"Frequently asked questions","lvl2":"Q: How can I get help?"},"content":"A: Use the built-in support link in the user area or email us at \n\nservice+tenants​-hub​-support​-request@sd​.eox​.at","type":"content","url":"/faq#q-how-can-i-get-help","position":15},{"hierarchy":{"lvl1":"Frequently asked questions","lvl2":"Q: Can I suggest a feature or report a bug?"},"type":"lvl2","url":"/faq#q-can-i-suggest-a-feature-or-report-a-bug","position":16},{"hierarchy":{"lvl1":"Frequently asked questions","lvl2":"Q: Can I suggest a feature or report a bug?"},"content":"A: Absolutely! We welcome feedback — either via email your workspace support panel.","type":"content","url":"/faq#q-can-i-suggest-a-feature-or-report-a-bug","position":17},{"hierarchy":{"lvl1":"Network of Resources"},"type":"lvl1","url":"/nor","position":0},{"hierarchy":{"lvl1":"Network of Resources"},"content":"","type":"content","url":"/nor","position":1},{"hierarchy":{"lvl1":"Network of Resources","lvl2":"Get ESA Sponsoring"},"type":"lvl2","url":"/nor#get-esa-sponsoring","position":2},{"hierarchy":{"lvl1":"Network of Resources","lvl2":"Get ESA Sponsoring"},"content":"You can access some of EOxHub Workspaces, API services, and data resources via ESA sponsoring from the Network of Resources (NoR). The NoR will provide successful applicants with a voucher for the selected services, allowing free-at-point-of-use consumption for research, product development, and up to pre-commercial demonstration.","type":"content","url":"/nor#get-esa-sponsoring","position":3},{"hierarchy":{"lvl1":"Network of Resources","lvl2":"Procedure"},"type":"lvl2","url":"/nor#procedure","position":4},{"hierarchy":{"lvl1":"Network of Resources","lvl2":"Procedure"},"content":"The most straightforward process is via the Network of resources Discovery Portal. You can check yourself which Euro Data Cube/EOxHub resources are currently available for sponsoring at \n\nhttps://​nor​-discover​.org/.\n\n\n\nFollow the step by step process via the corresponding Pricing Wizard. When done, export the proposal in PDF, sign it and email it to ESA.\n\n\n\nYou can always contact us via the \n\nEOxHub website to ask for support with requesting the right sponsorship for your project.\n\nAs additional (optional) step you can follow the Euro Data Cube check-out process to request preview functionality for immediate use (e.g. via a Promotional Free Plan) until your sponsoring request is confirmed.","type":"content","url":"/nor#procedure","position":5},{"hierarchy":{"lvl1":"How to create an account in EOxHub workspaces"},"type":"lvl1","url":"/creating-account","position":0},{"hierarchy":{"lvl1":"How to create an account in EOxHub workspaces"},"content":"1. Navigate to your EOxHub workspace. For example \n\nhttps://​gtif​-austria​.info/ .\n\n2. Proceed by clicking on the “Log in” button. In this case it is at the top right of your screen.\n\n3. You log into our services through GitHub. To continue, click the field titled “GitHub”.\n\n4. Sign into your GitHub account. If you do not own one, you will have to create one here.\n\n5. Keep in mind that not all EOxHub workspaces will automatically authorize your account. Sometimes you may have to wait to be approved by an administrator. Once you have completed all of that, you have successfully created an EOxHub account. Feel free to take a look at our other tutorials in order to understand how to utilize our tools.","type":"content","url":"/creating-account","position":1},{"hierarchy":{"lvl1":"Integrating GeoJSON dataset using Data Editor"},"type":"lvl1","url":"/geojson-tutorial","position":0},{"hierarchy":{"lvl1":"Integrating GeoJSON dataset using Data Editor"},"content":"","type":"content","url":"/geojson-tutorial","position":1},{"hierarchy":{"lvl1":"Integrating GeoJSON dataset using Data Editor","lvl2":"Integrating a dataset from small GeoJSON file into the gtif-austria.info service"},"type":"lvl2","url":"/geojson-tutorial#integrating-a-dataset-from-small-geojson-file-into-the-gtif-austria-info-service","position":2},{"hierarchy":{"lvl1":"Integrating GeoJSON dataset using Data Editor","lvl2":"Integrating a dataset from small GeoJSON file into the gtif-austria.info service"},"content":"1. Navigate to \n\nhttps://​gtif​-austria​.info\n\n2. Click “Log in”\n\n3. If it is the first time visiting the workspace you will need to register.\n\n4. After registration you are forwarded to the workspace. Click “Data Editor”\n\n5. If it is your first visit to the data editor it needs to be linked to your github account. Click “Authorize GTIF Austria Data Editor”\n\n6. Click on your username\n\n7. Click “Install & Authorize”. It is important to select “all repositories” as git-clerk will create fork from existing data catalog.\n\n8. Click “Start New Session”\n\n9. Click the “Session Name” field.\n\n10. Type how you want to call your session, e.g.  “new_resource_tutorial”\n\n11. Click “Create”\n\n12. Click “Create Dataset Submission”\n\n13. Click here.\n\n14. Type name you want to use for your data e.g. “geojson_example”\n\n15. Click “Submit”\n\n16. Click here.\n\n17. Type title you want to give your dataset, e.g. “GeoJSON Example”\n\n18. Click here.\n\n19. Type identifier you want to appear in the url when selected, e.g. “geojson_example1”\n\n20. Click here.\n\n21. Type the description for your dataset, (as markdown) e.g. “# Great description”\n\n22. Click on Resources\n\n23. Activate checkbox\n\n24. Click plus symbol to add a resource.\n\n25. Select type of resource, in this case “GeoJSON Source”\n\n26. Click here.\n\n27. Add “GeoJSON source” as Name (this will be autofilled in the future)\n\n28. Add the url of pre-created eodash JSON style file, see also \n\nhttps://​eodash​.org​/styling​.html for more information.\n\n(Dedicated tutorial on style definition being worked on)\n\n29. If you want the dashboard to zoom to a specific area you can select a bounding box on the map\n\n30. The geojson source assignes files to times, click on the plus to create a new time entry.\n\n31. Click here.\n\n32. Type a time string in ISO 8691 (\n\nISO 8601), e.g. “2025”\n\n33. Add an asset for that time, click on the plus button\n\n34. Type an identifier for the asset you want to use and add the public url of your geoJSON file in the Field entry\n\n35. Disable optional parameters by unchecking them (this step will not be needed in the future, they will be auto disabled)\n\n36. Click here.\n\n37. Click here.\n\n38. Click “Save”\n\n39. If the inputs were correct you should see “Preview creation is currently running...”\n\n40. After some time the preview should load, you can click on “Select indicator” to preview the newly configured dataset.\n\n41. Select the dataset you configured.\n\n42. Close the Selection panel by clicking “✕” (you can also use the fullscreen button to change from mobile view mode)\n\n43. Congratulations! You should see your data styled and to the area you selected!","type":"content","url":"/geojson-tutorial#integrating-a-dataset-from-small-geojson-file-into-the-gtif-austria-info-service","position":3},{"hierarchy":{"lvl1":"Add a new Indicator in Data Editor to show multiple datasets as layers in one visualisation"},"type":"lvl1","url":"/indicator-tutorial","position":0},{"hierarchy":{"lvl1":"Add a new Indicator in Data Editor to show multiple datasets as layers in one visualisation"},"content":"This guide provides a step-by-step process for merging multiple datasets into a single display collection using the EOxHub Data Editor.\nIt documents the task of adding a new Indicator file based on existing Collection files created beforehand. It also documents usage of additional Indicator specific properties Disable and Hidden.\n\n1. This tutorial expects already existing Collection files in the session, which are supposed to be grouped together in an Indicator file. Each collection will be shown as a layer in the final visualisation.\n\n2. Navigate to Data Editor in the workspace page e.g. \n\nhttps://​workspace​.gtif​-austria​.hub​-otc​.eox​.at​/catalog​-git​-clerk\n\n3. Select the active session which contains already created Collection files, here called “test-session”.\n\n4. Click “Browse Files” to navigate to the folder with indicators definitions.\n\nNotice existing pre-created files in collections. Their filenames will be used later.\n\n5. Click “indicators”\n\n6. Click “Add File” and “Add File here” to create a new indicator definition.\n\n7. Select “Edit in current session”\n\n8. Type the filename of a new indicator file with .json prefix, e.g. “testing-indicator-new.json”.\n\nClick on Add New File\n\n9. Insert mandatory fields Identifier and Title for the Indicator.\n\n10. Add mandatory information about which Collections are linked in this indicator.\n\n11. Click on the “+” icon to add a new link to a collection.\n\n12. Each Item is a filename of a Collection without the .json suffix e.g. LULUCF_land_use_copy_test\n\n13. Insert all Collection filenames which are supposed to be added.\n\n14. Click here.\n\n15. Metadata fields such as Themes defined on the Collections are not propagated into the Indicator so for indicators they must be defined here as well.\n\n16. Click on the “+” icon to add a new Theme and type it into the field.\n\n17. Two special fields for Indicators are Disable and Hidden.Click on the checkbox to activate the field.\n\nDisable is an optional list of Collections which should load as initially disabled in the dashboard layer control.Hidden is an optional list of Collections which should be completely hidden from the layer control but still appear on the map.\n\n18. Click on the “+” icon to add a new item and write in the filename of a collection without the file suffix to disable it.\n\n19. Same can be done for Hidden category. Click on edit properties to add Hidden by clicking on its checkbox. Then switch to the Hidden tab.\n\n20. Click on the “+” icon to add a new item and write in the filename of a collection without the file suffix to hide it from layercontrol.\n\n21. Click “Save” to start generation of an updated testing catalog.\n\n22. If on a smaller screen, in order to see the preview instance, click on the “Eye” icon\n\n23. “Preview creation takes a few minutes to finish”\n\n24. Once the preview generation finishes successfully, you can select the Indicator to check the visualization in the Tools section.\n\n25. Click “Select indicator”\n\n26. Search for your indicator Title to filter the list and select it.\n\n27. Click the newly added Indicator to open the visualisation.\n\n28. Two collections are displayed by default: Land Usage raster and NUTS geometries. The third collections defined in the Hidden is not shown.\n\n29. In the “Layers” tab,, you can confirm that the third Collection is indeed Hidden.\n\n30. You can then submit the session for review by clicking on the session name e.g. testing-session\n\n31. Request review.\n\n32. Click “Request button”\n\n33. Indicator file needs to be added manually to the catalog definition file e.g. “catalogs/gtif-austria.json” in order to appear in the platform after merging the underlying changes from the session.\n\nClick on the indicator file if any changes were already done, otherwise use the Browse Files and navigate to the catalogs folder.\n\n34. Edit the collections category.\n\n35. Click on a “+” icon to add a new reference to the Indicator.\n\n36. Scroll down to the bottom of the list.\n\n37. Add the filename of the new Indicator file without the suffix.\n\n38. Click “Save”","type":"content","url":"/indicator-tutorial","position":1},{"hierarchy":{"lvl1":"An introduction to the Narrative Editor"},"type":"lvl1","url":"/introduction-narrative-editor","position":0},{"hierarchy":{"lvl1":"An introduction to the Narrative Editor"},"content":"1. Navigate to your EOxHub workspace, for example \n\nhttps://​gtif​-austria​.info/ and log into your account. If you need help doing so, or do not yet own an account, follow the link to find the tutorial covering how to create one: \n\nHow to create an account in EOxHub workspaces\n\n2. After successfully signing in, find the “Narrative Editor” tile and click on it.\n\n3. If this is your first time using our Narrative Editor, you will have to authorize our tool to interact with your GitHub account.\n\n4. After doing so, make sure “All repositories” is selected and click “Install & Authorize”. The Narrative Editor requires access to all repositories to be able to create new narratives. It will not however, edit any of your other repositories.\n\n5. In order to create a narrative, you will have to create a session first. Sessions allow you to edit multiple files in one place. You will create your narrative within. To create a session click “Start New Session” towards the top right of your screen.\n\n6. Your session requires a name, which will be used to more easily find your work on GitHub. After typing in a name, click “Create New Session”.\n\n7. Select “Create Narrative” to create a new narrative. The other two options let you edit existing narratives or upload your own. Both without actually making changes to the original Website yet.\n\n8. Be sure to give your narrative a name and click “Submit”. This will create a new branch in your linked GitHub account and create a basic template.\n\n9. This template is only in place to get you started. Feel free to change anything you like.\n\n10. The title of your narrative, which is also shown on your hero image, is defined with a single hashtag “#”, while each new section in your narrative uses “##”. You can add a subheading to a section, by adding another hashtag: “###”. In total you can add up to four subheadings, by adding hashtags.\n\n11. The “Hide Preview”/“Show Preview” button at the top allows you to view your changes as you are making them.\n\n12. The little “+” icons let you add in new sections, such as paragraphs, maps, images and gifs, without having to type out the markdown syntax manually. Adding a new section, using “##”, will also add a new “+” icon in the preview section. Subheadings do not share this behaviour.\n\n13. Click the “Save” button at the top right of your screen to save your progress along the way.\n\n14. Once you are done creating or feel like starting a new session, click the “All Sessions” text in the top left corner of the editor. Doing so will return you to the menu displaying all of your sessions.\n\n15. The icons on the right side of your session overview let you view your session on GitHub, request your work to be reviewed, or delete the session entirely. Once you are ready for your work to be published, click the middle icon in order to send a review request.\n\n16. Confirm your request. This will create a pull request in GitHub, allowing you to view your work and any changes proposed by the review team.\n\n17. Click the icon shown on screen to view your session in GitHub.\n\n18. Click the link titled “View your narrative changes preview”.\n\n19. Scroll down to your narrative and click on it to open it.\n\n20. You will arrive at a preview of how your narrative would look once published. Now you just need to wait for your request to publish your narrative to be accepted.","type":"content","url":"/introduction-narrative-editor","position":1},{"hierarchy":{"lvl1":"Creating a Narrative (Story) in the EOxHub Workspace"},"type":"lvl1","url":"/narrative-editor-1","position":0},{"hierarchy":{"lvl1":"Creating a Narrative (Story) in the EOxHub Workspace"},"content":"1. Navigate to your workspace where Narrative Editor is available\n\n2. Click on Narrative Editor box\n\n3. If it is your first visit to the Narrative editor it needs to be linked to your github account. Click “Authorize Narrative Editor”\n\n4. Click on your username\n\n5. Select All repositories and click Install and Authorize. All repositories are needed as the tool is creating fork under your user.\n\n6. Click “Start New Session”\n\n7. Fill in the “Session Name” field. It is not a name of the story, but for the session and GitHub connection\n\n8. Click “Create”\n\n9. Click “Create Narrative” to start a new Narrative creation\n\n10. Here fill the Narrative Title - this will actually be used inside the story and also metadata\n\n11. Here is is named “Example Story”\n\n12. Click “Submit”\n\n13. Basic story template is created including the Title\n\n14. Eye Icon will open the Preview which can be used to visually confirm the Narrative status\n\n15. Some editing and Section adding can be done also in the Preview Mode\n\n16. Preview can be closed again by clicking on Eye icon\n\n17. You can add your content using markdown syntax and additional features like map sections from eodash dashboards. Full list of available features can be found here: \n\nhttps://​eodash​.org​/storytelling​.html and in the example story: \n\nexample-story.md\n\n18. Again Map Tour sections can be checked in Preview\n\n19.\n\n20. Click “Save” to save your content. This doesn’t have to be final version - you will be able to continue working in the same session until Narrative is ready for publishing\n\n21. Once the story is finalized, it can be submitted for review. Click on “folder” icon to get to home location\n\n22. “Request Review” Icon click will start the Review process on GitHub\n\n23. Confirm “Request”\n\n24. You can follow the Review process on GitHub - to do that, use GitHub icon\n\n25. GitHub Pull Request with the new story will open and reviewers and other contributors can be tagged. Story can be modified and once approved merged and displayed in the official Narrative catalog","type":"content","url":"/narrative-editor-1","position":1},{"hierarchy":{"lvl1":"Narrative Editor: Simple Map, Map Tour and a Chart"},"type":"lvl1","url":"/narrative-editor-maptour","position":0},{"hierarchy":{"lvl1":"Narrative Editor: Simple Map, Map Tour and a Chart"},"content":"1. Open your desired narrative.\n\n2. Click on a “+” icon in the preview area to add new sections to your narrative.\n\n3. Click “Simple Map” to add a map to your narrative.\n\n4. Name your map and click “Add Section”. Don’t worry about changing the text in the “Layers” Box.\n\n5. Changing the zoom value (in this case: 2) will alter how much your map will be magnified.\n\n6. Changing the coordinates to the right of “center” will change the focus point of your map. The left coordinate (in this case: 15) is used to change the longitude (East-West) while the right coordinate (in this case: 48) changes the latitude (North-South). Use a dot to add decimal values.\n\n7. To add a map tour, repeat the previous steps, but click “Tour” instead.\n\n8. Map tours use the same parameters as simple maps.\n\n9. Each new section in your narrative starts with “##”, while each subheading, or in this case, map tour step, starts with “###”. A title for said tour step adds another hashtag: “####”.\n\n10. To add another tour step, copy the code from the first tour step and paste it below the description of the first tour step. Make sure to add the “###”.\n\n11. To add a text box in the second tour step, add “####” and a title for the box (in this case: “Tour Title 2”\n\n12. Of course you can add in other maps as well.\n\n13. To do so, navigate to the homepage of your browser and click on “Explore data”.\n\n14. Select your desired dataset to implement into your narrative.\n\n15. We will be using the one shown one screen for this tutorial. However, the process is the same for all of the datasets.\n\n16. Click “Extract Storytelling configuration” to the left of the description.\n\n17. Select, what kind of map you would like to copy. In this case we will be implementing a simple map.\n\n18. Return to your narrative and paste in the text you just copied. Note that you can only add a map Tour section into an already existing map tour.\n\n19. Do not forget to click the “Show Preview” button at the top of the editor to view your changes.\n\n20. For datasets where processing results are visualized as a chart, you can export it as eox-chart block.\n\nClick on “copy as chart”.\n\n21. Paste it in the Narrative Editor. You can modify the height as fits your usecase.","type":"content","url":"/narrative-editor-maptour","position":1},{"hierarchy":{"lvl1":"Exploring Polar Warp Capability in eodash"},"type":"lvl1","url":"/polar-warp","position":0},{"hierarchy":{"lvl1":"Exploring Polar Warp Capability in eodash"},"content":"1. Navigate to \n\nhttps://​deside​.eox​.at/\n\n2. Click “Dashboard”\n\n3. Click “Select indicator”\n\n4. Select “Polar Warp Capability”\n\n5. Zoom in to location of your interest. Higher zoom level is needed to display Sentinel-1 images.\n\n6. Sentinel-1 layer in left panel can be used to find suitable time.\n\n7. Once preferred location is found, drawing rectangle and selecting additional parameters is needed for triggering process.\n\n8. Click “Execute”\n\n9. Running of capability can take couple minutes, depending on the size of the area.\n\n10. Move through predicted images in the left panel\n\n11.","type":"content","url":"/polar-warp","position":1},{"hierarchy":{"lvl1":"How to explore POLARIS results in eodash"},"type":"lvl1","url":"/polaris","position":0},{"hierarchy":{"lvl1":"How to explore POLARIS results in eodash"},"content":"1. Navigate to \n\nhttps://​deside​.eox​.at/\n\n2. Click “Dashboard”\n\n3. Click “Select indicator”\n\n4. Click “POLARIS”\n\n5. POLARIS options are displayed in the left panel\n\n6. You can select Type of Visualization, Ship Class and Type of Ice\n\n7. Additional information are displayed on mouse hover\n\n8. Different information is available for different visualization\n\n9. Date selection allows comparison","type":"content","url":"/polaris","position":1},{"hierarchy":{"lvl1":"Publishing Data workflow"},"type":"lvl1","url":"/publishing-workflow-tutorial","position":0},{"hierarchy":{"lvl1":"Publishing Data workflow"},"content":"This tutorial explains the complete workflow for preparing, uploading, styling, and publishing datasets in an EOXHub Workspace which includes multiple of the available Applications within EOxHub. It is intended as overview and does not go too deep into the options within the various steps, but provides follow on resources if more information is needed. The same steps apply across all workspace instances (e.g., GTIF Austria, Cerulean, and other environments).\n\nFor more information on the dedicated Applications, please further explore this documentation.\n\nNote\n\nThe Publishing of data in the Dashboard is only available for workspaces that have added the paid option Dashboard as a Service (DaaS)","type":"content","url":"/publishing-workflow-tutorial","position":1},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl2":"Prerequisites"},"type":"lvl2","url":"/publishing-workflow-tutorial#prerequisites","position":2},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl2":"Prerequisites"},"content":"In order to submit a data publishing request for raw resource, i.e. a data format that can directly be visualized by the dashboard (Cloud optimized GeoTIFF (COG), GeoJSON, FlatgeoBuf) you need following things:\n\nHave a geo file that is accessible through public URL\n\nHave a style definition file that is accessible through public URL\n\nThe tutorial covers how to achieve the prerequisites within an EOxHub Workspace environment and various other aspects of the Data Publishing submission:\n\nUploading data with the File Browser\n\nDefining style (color scales, legends, no-data handling)\n\nCreating and configuring datasets in the Data Editor\n\nUsing Cloud-Optimized GeoTIFF (COG) sources\n\nCombining multiple datasets into one indicator\n\nAdding previews and legends","type":"content","url":"/publishing-workflow-tutorial#prerequisites","position":3},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl2":"1. Uploading Data with the File Browser"},"type":"lvl2","url":"/publishing-workflow-tutorial#id-1-uploading-data-with-the-file-browser","position":4},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl2":"1. Uploading Data with the File Browser"},"content":"For the first prerequisite (publicly available geo-file) it is possible to use the File browser application. The \n\nFile Browser allows adding data to a workspace.\n\nA special public folder is available where you can upload the files which are expected to be published.\nYou can upload files (Tiff (Cloud Optimized Geotiff - COG), GeoJSON, style files, preview images, etc.) directly in the browser.\n\nOnce a file is uploaded it can be accessed through a special URL, similar to following:https://workspace-ui-public.<instance>.hub.eox.at/api/public/share/public/<filename>.tif\n\nThe exact url as well as a short description is provided within the README.txt inside the public folder of your workspace.\nPresigned URLs generated from the File Browser are temporary and should not be used in the Dashboard configuration. Always use the permanent public URL.\n\n⚠️ Upload through the browser to the workspace storage has some size limitations, files over ~100 MB should be uploaded differently.","type":"content","url":"/publishing-workflow-tutorial#id-1-uploading-data-with-the-file-browser","position":5},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl3":"Raw Data formats","lvl2":"1. Uploading Data with the File Browser"},"type":"lvl3","url":"/publishing-workflow-tutorial#raw-data-formats","position":6},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl3":"Raw Data formats","lvl2":"1. Uploading Data with the File Browser"},"content":"Here is more information on the supported formats:\n\nRaster Data:\n\nUse Cloud-Optimized GeoTIFF (COG)\n\nIdeally in EPSG:3857 projection for best support performance\n\nOther projections should also work but have a performance penalty when being visualized\n\nCan be a single file or a time series (multiple files).\n\nIdeally encode the date and time in the filename using ISO format: YYYY-MM-DDTHH:MM:SS.\n\nBands:\n\nRGB bands (already prepared as final visualization), or\n\nData bands in which case a style will be needed (see section 3.)\n\nVector Data:\n\nUse GeoJSON if the file size is under ~10 MB, or FlatGeobuf for larger datasets\n\nEven larger datasets should be handled differently, will need dedicated tile server\n\nWill need a style definition as described in section 3.\n\nFiles in the public folder are openly accessible and can be integrated into the Dashboard.\n\nEach instance (where available) provides permanent public URLs for assets.\n\nAdditional information on resource properties, such as raw data source can be found in the eodash_catalog wiki for \n\nResource.","type":"content","url":"/publishing-workflow-tutorial#raw-data-formats","position":7},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl2":"2. Style Definition"},"type":"lvl2","url":"/publishing-workflow-tutorial#id-2-style-definition","position":8},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl2":"2. Style Definition"},"content":"COGs with numeric values (e.g., temperature) as well as vector data (GeoJSON, FlatGeobuf) require a style definition so that it is clear how they should be visualized.\n\nThis section covers handling the second pre-requisite, creating and making the style available online.","type":"content","url":"/publishing-workflow-tutorial#id-2-style-definition","position":9},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl3":"Style creation","lvl2":"2. Style Definition"},"type":"lvl3","url":"/publishing-workflow-tutorial#style-creation","position":10},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl3":"Style creation","lvl2":"2. Style Definition"},"content":"The style is based on \n\nOpenLayers expressions using \n\nOpenLayers flat style definition.\n\nFurther information on styling in the eodash client can be found in the \n\neodash documentation.\n\nAn experimental client to help quicker iterate in the style definition is available under \n\nhttps://​eodash​.github​.io​/eodash​-style​-editor/. There you can set a URL for your dataset and work on the style definition live. Please take into account this is not a fully functioning experimental helper tool. A more streamlined integration is envisioned and the deployment of the current tool will probably change.\n\nExample: Temperature color scale with no-data handling:{\n  \"color\": [\n    \"case\",\n    [\">\", [\"band\", 1], 0],\n    [\n      \"interpolate\",\n      [\"linear\"],\n      [\"band\", 1],\n      290,\n      [0, 0, 255, 1],\n      310,\n      [253, 0, 0, 1]\n    ],\n    [0, 0, 0, 0]\n  ]\n}\n\n[255, 255, 255, 1] = RGBA values (RGB: 0–255, Alpha: 0–1).\n\n[\"band\", 1] = first band of the COG.\n\n\"interpolate\" = smooth gradient between values.\n\n❌ JSON does not support comments (// ...). Make sure to not use them in the definition or parsing will fail.","type":"content","url":"/publishing-workflow-tutorial#style-creation","position":11},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl3":"Style online deployment","lvl2":"2. Style Definition"},"type":"lvl3","url":"/publishing-workflow-tutorial#style-online-deployment","position":12},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl3":"Style online deployment","lvl2":"2. Style Definition"},"content":"Using the same logic as making the cloud optimized file available online, we can use the File Browser to upload or directly create the style json file to a location within the public folder.\nIf you want to create the style and copy the style configuration content from the style editor, you can click on “New file” 📄, name it for example style.json, and in the opened editor paste your configuration. Make sure to click on save button 💾 once done.\nThen you can use the public endpoint as explained previously.\n\nThere are of course other ways of making a file public, many services exist especially for a json format. Style files can become intricate depending on the use case or done as collaboration activity so it might be beneficial to use a service that provides change tracking.","type":"content","url":"/publishing-workflow-tutorial#style-online-deployment","position":13},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl2":"3. Submitting a data publishing request"},"type":"lvl2","url":"/publishing-workflow-tutorial#id-3-submitting-a-data-publishing-request","position":14},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl2":"3. Submitting a data publishing request"},"content":"Once your files are uploaded, you need to register them in the Data Editor.\n\nThe basic steps are:\n\nGo to Data Editor → Start New Session.\n\nAutomation → Create Dataset Submission:\n\nEnter Data Title and click Submit\n\nInside opened form:\n\nMake sure identifier has no special characters or white spaces\n\nUnder Resources, click on the plus (+) sign\n\nUnder item 1 select Cloud Optimized Geotiff (COG) Source.\n\nAdd to the Style field the URL to a style definition (prerequisite 2 - as described in section 2 previously)\n\nDraw a bounding box dashboard should zoom to when dataset selected (click → move mouse → click again to define rectangle)\n\nClick plus (+) TimeEntries\n\nWrite a Time string in \n\nISO 8601 format, e.g. YYYY-MM-DD or just year YYYY\n\nClick on the plus (+) sign of the Assets field\n\nAdd an identifier string, e.g. data\n\nPaste the URL to your public file into the File field (prerequisite 1 - as described in section 1 previously)\n\nExample entry for a COG resource:{\n  \"time\": \"2019-06-27T00:00:00Z\",\n  \"assets\": {\n    \"file\": \"https://workspace-ui-public.example.hub.eox.at/api/public/share/public/example.tif\"\n  }\n}\n\nThis is intended as short summary, for more detailed guide with screenshots please look at \n\nIntegrating GeoJSON dataset using Data Editor, where the shown steps are applicable to all raw resource submissions.","type":"content","url":"/publishing-workflow-tutorial#id-3-submitting-a-data-publishing-request","position":15},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl3":"Adding a Legend","lvl2":"3. Submitting a data publishing request"},"type":"lvl3","url":"/publishing-workflow-tutorial#adding-a-legend","position":16},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl3":"Adding a Legend","lvl2":"3. Submitting a data publishing request"},"content":"There are in principle three approaches for defining a legend.\n\nUsing legend property within the style definition\n\nIf your style utilizes variables and allows dynamic changes, e.g. value range change, this is the best alternative, as it allows to bind the legend to the variables, so it will update according to user changes. For a static legend you can use following in your style:{\n  \"color\": {...}, // full color definition to be filled\n  \"legend\": {\n    \"title\": \"Title text\",\n    \"range\": [\n      \"rgba(0, 0, 255, 1)\",\n      \"rgba(170, 170, 170, 1)\",\n      \"rgba(255, 0, 0, 1)\"\n    ],\n    \"domain\": [0, 10]\n  },\n   \"jsonform\": {\n        \"type\": \"object\",\n        \"title\": \"Data configuration\",\n        \"properties\": {}\n   }\n}\n\nFor dynamic changes instead of “domain”, “domainProperties” can be used:{\n  \"color\": {...}, // full color definition to be filled\n  \"legend\": {\n    \"title\": \"Title text\",\n    \"range\": [\n      \"rgba(0, 0, 255, 1)\",\n      \"rgba(170, 170, 170, 1)\",\n      \"rgba(255, 0, 0, 1)\"\n    ],\n    \"domainProperties\": [\"vmin\", \"vmax\"]\n  },\n  \"variables\": {\n    \"vmin\": 2,\n    \"vmax\": 10\n  },\n  \"jsonform\": {\n    \"type\": \"object\",\n    \"title\": \"Data configuration\",\n    \"properties\": {...} // full definition of form fields to manipulate variables\n   }\n}\n\nApart from that it is possible to:\n\nProvide Colorlegend in collection definition\n\nAdd the Colorlegend property for a collection in the Data Editor (or collection json definition). \n\nHere is more information on the properties.\n\nLegend in collection definition\n\nIf the legend needs to be completely custom it is possible to use an image. The URL to the image can be specified in the optional Legend property of the collection in the Data Editor.","type":"content","url":"/publishing-workflow-tutorial#adding-a-legend","position":17},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl3":"Combining Multiple Datasets","lvl2":"3. Submitting a data publishing request"},"type":"lvl3","url":"/publishing-workflow-tutorial#combining-multiple-datasets","position":18},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl3":"Combining Multiple Datasets","lvl2":"3. Submitting a data publishing request"},"content":"If multiple datasets should be shown together in the dashboard, there are mainly two options:","type":"content","url":"/publishing-workflow-tutorial#combining-multiple-datasets","position":19},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl4":"Option A: Collections under one Indicator","lvl3":"Combining Multiple Datasets","lvl2":"3. Submitting a data publishing request"},"type":"lvl4","url":"/publishing-workflow-tutorial#option-a-collections-under-one-indicator","position":20},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl4":"Option A: Collections under one Indicator","lvl3":"Combining Multiple Datasets","lvl2":"3. Submitting a data publishing request"},"content":"Create two Collections (e.g., temperature, cooling degree days).\n\nCombine them under one Indicator definition.\n\nUpdate the catalog.json to reference the new indicator.\n\nIt is possible to use the “Browse Files” button in the Data Editor to find and modify most text based files. As alternative the changes can be made directly into the github repository if that is considered an easier alternative.\n\nExample indicator (simplified):{\n  \"id\": \"combined_indicator\",\n  \"collections\": [\"temperature_collection\", \"cooling_degree_collection\"]\n}","type":"content","url":"/publishing-workflow-tutorial#option-a-collections-under-one-indicator","position":21},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl4":"Option B: Multiple Assets for one Time Entry","lvl3":"Combining Multiple Datasets","lvl2":"3. Submitting a data publishing request"},"type":"lvl4","url":"/publishing-workflow-tutorial#option-b-multiple-assets-for-one-time-entry","position":22},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl4":"Option B: Multiple Assets for one Time Entry","lvl3":"Combining Multiple Datasets","lvl2":"3. Submitting a data publishing request"},"content":"If both datasets share the same spatial/temporal extent:\n\nAdd multiple assets inside one time entry\n\nAccess via [\"band\", 1], [\"band\", 2] in your style\n\nThis allows very interesting dynamic band arithmetic and filtering options combining datasets.\n\nExample json structure:\"TimeEntries\": [\n  {\n    \"Time\": \"20250101\",\n    \"Assets\": [\n      // equivalent to [\"band\", 1]\n      {\n        \"Identifier\": \"temperature\",\n        \"File\": \"https://workspace-ui-public.gtif-austria.hub-otc.eox.at/api/public/temperature2025.tiff\"\n      },\n      // equivalent to [\"band\", 2]\n      {\n        \"Identifier\": \"humidity\",\n        \"File\": \"https://workspace-ui-public.gtif-austria.hub-otc.eox.at/api/public/humidity2025.tiff\"\n      }\n    ]\n  }\n]","type":"content","url":"/publishing-workflow-tutorial#option-b-multiple-assets-for-one-time-entry","position":23},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl3":"Adding a Preview Image","lvl2":"3. Submitting a data publishing request"},"type":"lvl3","url":"/publishing-workflow-tutorial#adding-a-preview-image","position":24},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl3":"Adding a Preview Image","lvl2":"3. Submitting a data publishing request"},"content":"You can add a preview image under General → Image. For hosting the image you can use for example the File Browser to upload it to the public folder.\n\nExample:\"image\": \"https://workspace-ui-public.example.hub.eox.at/api/public/assets/preview.jpg\"\n\n⚠️ Make sure:\n\nThe preview file is uploaded to the public folder.\n\nYou use the full URL (not a relative path).","type":"content","url":"/publishing-workflow-tutorial#adding-a-preview-image","position":25},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl2":"6. Publishing the Dataset"},"type":"lvl2","url":"/publishing-workflow-tutorial#id-6-publishing-the-dataset","position":26},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl2":"6. Publishing the Dataset"},"content":"Workflow:\n\nWork in a test session in the Data Editor.\n\nValidate dataset and style in the Preview.\n\nMark the session as Ready for Review.\n\nOnce approved, the session is merged into the public catalog and displayed in the associated public dashboard.","type":"content","url":"/publishing-workflow-tutorial#id-6-publishing-the-dataset","position":27},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl2":"7. Troubleshooting"},"type":"lvl2","url":"/publishing-workflow-tutorial#id-7-troubleshooting","position":28},{"hierarchy":{"lvl1":"Publishing Data workflow","lvl2":"7. Troubleshooting"},"content":"Styles not working → Remove comments (//) from JSON.\n\nWrong colors displayed → Check style file URL, alpha values, and cache refresh.\n\nPreview image not visible → Ensure correct full URL, not a relative path.\n\nLegend missing → Make sure one of the 3 options described is used.\n\nFor any workspace external sources → Externally hosted resources often cannot be directly integrated into another web client unless the server hosting the resource sets the proper cross-origin (CORS) headers in its responses. You can verify this using online tools such as \n\nhttps://​cors​-test​.codehappy​.dev/ (not affiliated with this service—use at your own discretion).\n\nThis covers the main steps for publishing data by uploading, styling, combining, and publishing in an EOXHub Workspace.\nFor more information on the used Applications further explore the documentation.","type":"content","url":"/publishing-workflow-tutorial#id-7-troubleshooting","position":29},{"hierarchy":{"lvl1":"Integrating WMTS dataset using Data Editor"},"type":"lvl1","url":"/wmts-tutorial","position":0},{"hierarchy":{"lvl1":"Integrating WMTS dataset using Data Editor"},"content":"","type":"content","url":"/wmts-tutorial","position":1},{"hierarchy":{"lvl1":"Integrating WMTS dataset using Data Editor","lvl2":"Integrating a dataset from the Copernicus Marine Data Store into the eodashboard.org service"},"type":"lvl2","url":"/wmts-tutorial#integrating-a-dataset-from-the-copernicus-marine-data-store-into-the-eodashboard-org-service","position":2},{"hierarchy":{"lvl1":"Integrating WMTS dataset using Data Editor","lvl2":"Integrating a dataset from the Copernicus Marine Data Store into the eodashboard.org service"},"content":"1. Navigate to \n\nhttps://​data​.marine​.copernicus​.eu​/products\n\n2. Search for the dataset you are interested in, for example by using the Click the “Free text” search. (In order for free search to work it seems you need to add also a date range or it will get stuck), for example you can type “chlorophyll a” enter a time range.\n\n3. Let us continue the example wit “Global Ocean Colour”.\n\nClick “Global Ocean Colour (Copernicus-GlobColour), Bio-Geo-Chemical, L4 (monthly and interpolated) from Satellite Observations (Near Real Time)”\n\n4. Click “Data access”\n\n5. Click “WMTS”\n\n6. Use search within the WMTS xml file to find the [[<Layer]] you are interested in which includes the [[ows:Identifier]] you will need for the form later, as well as valuable  information in the [[VariableInformation]] and default style\n\n7. Go to EO Dashboard workspace \n\nhttp://​workspace​.eodashboard​.org/ and log-in.\n\nIf you do not have an account you need to register. Currently new users need to be manually authorized to get access\n\n8. Click “Data Editor”\n\n9. Click “Start New Session”\n\n10. Click the “Session Name” field.\n\n11. Type the session name you want to have e.g. “new_cmems_data”\n\n12. Click “Create”\n\n13. Click “Add/Edit File Manually”\n\n14. Click the “File Name” field.\n\n15. Type “col”\n\n16. Select the “collections” folder\n\n17. Type the name to use as collection file name (has to be a JSON file), e.g. “/cmems_data_1.json” (A new button will be introduced to simplify this step in the future)\n\n18. Click “Create”\n\n19. Start filling out the fields:\n\n20. Type “cmems_data_chl”\n\n21. Add a title\n\n22. Add an Eodashidentifier, it should be unique among the available collection, will be used in the URL to link to it directly e.g: ?indicator=CMEMS_CHL\n\n23. Optional properties can be enabled by clicking the check boxes\n\n24. Add some description for the dataset as markdown text\n\n25. Click to resources, in principle all other tabs are optional\n\n26. Click on the plus symbol\n\n27. Select the resource type, in this case “Copernicus Marine Data Store WMTS”\n\n28. Click on endpoint\n\n29. Copy the URL from the WMTS tab you opened earlier and remove the section starting from the question mark e.g. from \n\nhttps://​wmts​.marine​.copernicus​.eu​/teroWmts​/OCEANCOLOUR​_GLO​_BGC​_L4​_NRT​_009​_102​/cmems​_obs​-oc​_glo​_bgc​-plankton​_nrt​_l4​-olci​-300m​_P1M​_202211​?request​=​GetCapabilities​&​service​=​WMS to:\n\nhttps://​wmts​.marine​.copernicus​.eu​/teroWmts​/OCEANCOLOUR​_GLO​_BGC​_L4​_NRT​_009​_102​/cmems​_obs​-oc​_glo​_bgc​-plankton​_nrt​_l4​-olci​-300m​_P1M​_202211  and paste it into the field\n\n30. Click here.\n\n31. Type “marinedatastore” in the Name field as well as “WMTSCapabilities” in the Type field.\n\n32. Fill in the LayerID field\n\n33. You can enable the time query start and end parameter to only retrieve specific time extent, for example 2024-01-01T00:00:00Z to 2025-01-27T00:00:00Z\n\n34. The dimensions specification through the UI is still in progress\n\n35. Click “Save”\n\n36. After about 1 minute right-click “on the preview field” (probably in white area on the right side), search for Reload frame option and click it. If not found it is also possible to reload the whole page and navigate back to the file you were editing.\n\n37. Click to expand the preview (currently an issue that it falls back to mobile view)\n\n38. Click to select indicator\n\n39. You should see the indicator you just added, click on it\n\n40. You should see the preview of your newly integrated indicator\n\n41. The elevation should be specified as Dimension (currently not possible through UI) it defaults to some value (not 0) making the data rendering look different then in the CMEMS data explorer)","type":"content","url":"/wmts-tutorial#integrating-a-dataset-from-the-copernicus-marine-data-store-into-the-eodashboard-org-service","position":3},{"hierarchy":{"lvl1":"Algorithm as a Service"},"type":"lvl1","url":"/algorithm-as-a-service","position":0},{"hierarchy":{"lvl1":"Algorithm as a Service"},"content":"In this use case, algorithms are made available to others as callable services. Instead of running code manually, users can trigger processing via a web interface or API by providing input parameters (e.g., area of interest, start and end time).\n\nThis approach allows your logic to be reused consistently, integrated into dashboards, or shared with partners without exposing the underlying code. It’s ideal for creating operational tools and shared analytics infrastructure.\n\n🛠 Workspace tools:\n\nArgo Workflows can be exposed as callable jobs.\n\nHeadless Execution enables algorithms, mainly Argo Workflows to be published as OGC-compliant API endpoints.","type":"content","url":"/algorithm-as-a-service","position":1},{"hierarchy":{"lvl1":"Algorithm Development"},"type":"lvl1","url":"/algorithm-development","position":0},{"hierarchy":{"lvl1":"Algorithm Development"},"content":"This use case focuses on creating new methods for processing geospatial data. Users explore different techniques, write and test code, and visualize intermediate results to better understand patterns and relationships in EO data. The emphasis is on experimentation, flexibility, and rapid iteration.\n\nThis phase is particularly relevant for data scientists and researchers aiming to experiment with new techniques, machine learning models, or domain-specific indices. During this phase, the focus is not yet on scale or automation, but on designing something effective and accurate.\n\n🛠 Workspace tools:\n\nJupyterLab offers a flexible development environment with rich visualization, code execution, and data inspection features.\n\nConda Store allows you to build custom Python environments with specialized geospatial, ML, or scientific libraries tailored to your project.","type":"content","url":"/algorithm-development","position":1},{"hierarchy":{"lvl1":"Publish Insights"},"type":"lvl1","url":"/publish-insights","position":0},{"hierarchy":{"lvl1":"Publish Insights"},"content":"After data is processed and analyzed, the final step is to communicate the results in an accessible way. This use case focuses on turning technical outputs into engaging narratives or interactive dashboards that help stakeholders understand key findings and make informed decisions.\n\nInsights might be shared with policymakers, researchers, or the public through dashboards, map-based stories, or small reports. The goal is to present complex data in a visual and context-rich format.\n\nTo support the presentation of datasets, EOxHub includes a publishing layer that leverages tools such as \n\neodash. Operators can describe their datasets using structured JSON Schema based forms, which are translated into STAC metadata and managed via a Git-based flow using Data Editor and Narrative Editor.\n\nThis approach allows updates to be reviewed, versioned, and automatically reflected in the workspace data catalogs. Every editing session corresponds to a separate named branch in a personal GitHub fork of a target catalog repository.\n\n🛠 Workspace tools:\n\nNarrative Editor allows creation of scrollytelling stories that combine maps, markdown text, and data visualizations\n\nData Editor allows addition of data sets into the eodash dashboard\n\nJupyterLab can be used to generate plots, charts, and descriptions to support your story\n\nPublishing Dashboard build your own dashboard full of datasets and interactive visualizations.","type":"content","url":"/publish-insights","position":1},{"hierarchy":{"lvl1":"Result Generation"},"type":"lvl1","url":"/result-generation","position":0},{"hierarchy":{"lvl1":"Result Generation"},"content":"Once an algorithm has been designed and tested, it can be used to process large volumes of input data and generate consistent outputs. This can happen on-demand, on a schedule, or as part of a larger workflow. Result generation is particularly suited for monitoring applications, change detection, and batch processing of large-scale datasets. It includes mechanisms for task automation, parameterization, and reproducible execution.\n\nResult generation involves configuring the algorithm to run over different input areas or time periods. It can also include tracking job runs, managing outputs, and ensuring reproducibility.\n\n🛠 Workspace tools:\n\nArgo Workflows provide scalable, repeatable execution pipelines.\n\nHeadless Execution lets you run notebooks or workflows without direct user interaction, ideal for scheduled tasks.\n\nCredentials Manager ensures access to protected resources like cloud buckets or Sentinel Hub is securely handled.","type":"content","url":"/result-generation","position":1}]}